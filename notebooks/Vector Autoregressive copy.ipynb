{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T16:10:27.245384Z",
     "start_time": "2024-07-02T16:10:23.312318Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Functions\n",
    "# Add the directory containing the module to sys.path\n",
    "sys.path.append(os.path.abspath('functions'))\n",
    "from preprocessing import read_csv_files, check_missing_values, forward_fill, backward_fill, linear_interpolation, calculate_volatility, adf_test, check_date_range, extract_date_range, map_date_range, create_volatility_df, stationary_transformation\n",
    "from spillover import calculate_avg_spillover_table, calculate_net_pairwise_spillover_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a list of DataFrames\n",
    "dataframes = read_csv_files(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each column in every DataFrame to lowercase\n",
    "for key in dataframes:\n",
    "    dataframes[key].columns = map(str.lower, dataframes[key].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the `date` column to datetime object, this process has to be handled individually since each DataFrame has a different date format. We cannot let Pandas infers the date format for each DataFrame since it can be prone to infer the wrong format. Below are the format of each DataFrame:\n",
    "- Philippines: MM/DD/YYYY\n",
    "- Singapore: MM/DD/YYYY\n",
    "- India: YYYY-MM-DD\n",
    "- United Kingdom: DD/MM/YYYY\n",
    "- Mexico: YYYY-MM-DD\n",
    "- Japan: YYYY-MM-DD\n",
    "- Vietnam: DD/MM/YYYY\n",
    "- Korea: YYYY-MM-DD\n",
    "- Thailand: YYYY-MM-DD\n",
    "- Brazil: YYYY-MM-DD\n",
    "- Malaysia: DD/MM/YYYY\n",
    "- Switzerland: YYYY-MM-DD\n",
    "- China: DD/MM/YYYY\n",
    "- Russia: YYYY-MM-DD\n",
    "- United States: YYYY-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format_mapping = {\n",
    "  'philippines': '%m/%d/%Y',\n",
    "  'singapore': '%m/%d/%Y',\n",
    "  'india': '%Y-%m-%d',\n",
    "  'uk': '%d/%m/%Y',\n",
    "  'mexico': '%Y-%m-%d',\n",
    "  'japan': '%Y-%m-%d',\n",
    "  'vietnam': '%d/%m/%Y',\n",
    "  'korea': '%Y-%m-%d',\n",
    "  'thailand': '%Y-%m-%d',\n",
    "  'brazil': '%Y-%m-%d',\n",
    "  'malaysia': '%d/%m/%Y',\n",
    "  'switzerland': '%Y-%m-%d',\n",
    "  'china': '%d/%m/%Y',\n",
    "  'russia': '%Y-%m-%d',\n",
    "  'us': '%Y-%m-%d',\n",
    "}\n",
    "\n",
    "# Convert the date columns to datetime objects\n",
    "for key in dataframes:\n",
    "    try:\n",
    "      dataframes[key]['date'] = pd.to_datetime(\n",
    "        dataframes[key]['date'], \n",
    "        format=date_format_mapping[key]\n",
    "      )\n",
    "    except Exception as e:\n",
    "      print(f\"Error occurred for country: {key}\")\n",
    "      print(f\"Error message: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframes by date in ascending order\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the dataframes\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only open, high, low, close columns\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key][['date', 'open', 'high', 'low', 'close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3055 entries, 0 to 3054\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    3055 non-null   object\n",
      " 1   open    3055 non-null   object\n",
      " 2   high    3055 non-null   object\n",
      " 3   low     3055 non-null   object\n",
      " 4   close   3055 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 119.5+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3181 entries, 0 to 3180\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    3181 non-null   object\n",
      " 1   open    3181 non-null   object\n",
      " 2   high    3181 non-null   object\n",
      " 3   low     3181 non-null   object\n",
      " 4   close   3181 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 124.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4101 entries, 0 to 4100\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    4101 non-null   object \n",
      " 1   open    4062 non-null   float64\n",
      " 2   high    4062 non-null   float64\n",
      " 3   low     4062 non-null   float64\n",
      " 4   close   4062 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 160.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5597 entries, 0 to 5596\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    5597 non-null   object\n",
      " 1   open    5597 non-null   object\n",
      " 2   high    5597 non-null   object\n",
      " 3   low     5597 non-null   object\n",
      " 4   close   5597 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 218.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5656 entries, 0 to 5655\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    5656 non-null   object \n",
      " 1   open    5607 non-null   float64\n",
      " 2   high    5607 non-null   float64\n",
      " 3   low     5607 non-null   float64\n",
      " 4   close   5607 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 221.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5559 entries, 0 to 5558\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    5559 non-null   object \n",
      " 1   open    5466 non-null   float64\n",
      " 2   high    5466 non-null   float64\n",
      " 3   low     5466 non-null   float64\n",
      " 4   close   5466 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 217.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3821 entries, 0 to 3820\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    3821 non-null   object\n",
      " 1   open    3821 non-null   object\n",
      " 2   high    3821 non-null   object\n",
      " 3   low     3821 non-null   object\n",
      " 4   close   3821 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 149.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5578 entries, 0 to 5577\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    5578 non-null   object \n",
      " 1   open    5512 non-null   float64\n",
      " 2   high    5512 non-null   float64\n",
      " 3   low     5512 non-null   float64\n",
      " 4   close   5512 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 218.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5515 entries, 0 to 5514\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    5515 non-null   object \n",
      " 1   open    5442 non-null   float64\n",
      " 2   high    5442 non-null   float64\n",
      " 3   low     5442 non-null   float64\n",
      " 4   close   5442 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 215.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3522 entries, 0 to 3521\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    3522 non-null   object \n",
      " 1   open    3060 non-null   float64\n",
      " 2   high    3060 non-null   float64\n",
      " 3   low     3060 non-null   float64\n",
      " 4   close   3060 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 137.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3436 entries, 0 to 3435\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    3436 non-null   object\n",
      " 1   open    3436 non-null   object\n",
      " 2   high    3436 non-null   object\n",
      " 3   low     3436 non-null   object\n",
      " 4   close   3436 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 134.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5662 entries, 0 to 5661\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    5662 non-null   object \n",
      " 1   open    5610 non-null   float64\n",
      " 2   high    5610 non-null   float64\n",
      " 3   low     5610 non-null   float64\n",
      " 4   close   5610 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 221.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4697 entries, 0 to 4696\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    4697 non-null   object\n",
      " 1   open    4697 non-null   object\n",
      " 2   high    4697 non-null   object\n",
      " 3   low     4697 non-null   object\n",
      " 4   close   4697 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 183.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2788 entries, 0 to 2787\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    2788 non-null   object \n",
      " 1   open    2749 non-null   float64\n",
      " 2   high    2749 non-null   float64\n",
      " 3   low     2749 non-null   float64\n",
      " 4   close   2749 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 109.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5620 entries, 0 to 5619\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    5620 non-null   object \n",
      " 1   open    5620 non-null   float64\n",
      " 2   high    5620 non-null   float64\n",
      " 3   low     5620 non-null   float64\n",
      " 4   close   5620 non-null   float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 219.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for df in dataframes.values():\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the columns open, high, low, close have different dtypes (`float64` and `object`) for different DataFrame, they should be converted to `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert open, high, low, close columns to float\n",
    "for key in dataframes:\n",
    "  for col in ['open', 'high', 'low', 'close']:\n",
    "    if dataframes[key][col].dtype == 'object':\n",
    "      dataframes[key][col] = dataframes[key][col].str.replace(',', '').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3055 entries, 0 to 3054\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    3055 non-null   datetime64[ns]\n",
      " 1   open    3055 non-null   float64       \n",
      " 2   high    3055 non-null   float64       \n",
      " 3   low     3055 non-null   float64       \n",
      " 4   close   3055 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 119.5 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3181 entries, 0 to 3180\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    3181 non-null   datetime64[ns]\n",
      " 1   open    3181 non-null   float64       \n",
      " 2   high    3181 non-null   float64       \n",
      " 3   low     3181 non-null   float64       \n",
      " 4   close   3181 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 124.4 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4101 entries, 0 to 4100\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    4101 non-null   datetime64[ns]\n",
      " 1   open    4062 non-null   float64       \n",
      " 2   high    4062 non-null   float64       \n",
      " 3   low     4062 non-null   float64       \n",
      " 4   close   4062 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 160.3 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5597 entries, 0 to 5596\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    5597 non-null   datetime64[ns]\n",
      " 1   open    5597 non-null   float64       \n",
      " 2   high    5597 non-null   float64       \n",
      " 3   low     5597 non-null   float64       \n",
      " 4   close   5597 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 218.8 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5656 entries, 0 to 5655\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    5656 non-null   datetime64[ns]\n",
      " 1   open    5607 non-null   float64       \n",
      " 2   high    5607 non-null   float64       \n",
      " 3   low     5607 non-null   float64       \n",
      " 4   close   5607 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 221.1 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5559 entries, 0 to 5558\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    5559 non-null   datetime64[ns]\n",
      " 1   open    5466 non-null   float64       \n",
      " 2   high    5466 non-null   float64       \n",
      " 3   low     5466 non-null   float64       \n",
      " 4   close   5466 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 217.3 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3821 entries, 0 to 3820\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    3821 non-null   datetime64[ns]\n",
      " 1   open    3821 non-null   float64       \n",
      " 2   high    3821 non-null   float64       \n",
      " 3   low     3821 non-null   float64       \n",
      " 4   close   3821 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 149.4 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5578 entries, 0 to 5577\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    5578 non-null   datetime64[ns]\n",
      " 1   open    5512 non-null   float64       \n",
      " 2   high    5512 non-null   float64       \n",
      " 3   low     5512 non-null   float64       \n",
      " 4   close   5512 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 218.0 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5515 entries, 0 to 5514\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    5515 non-null   datetime64[ns]\n",
      " 1   open    5442 non-null   float64       \n",
      " 2   high    5442 non-null   float64       \n",
      " 3   low     5442 non-null   float64       \n",
      " 4   close   5442 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 215.6 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3522 entries, 0 to 3521\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    3522 non-null   datetime64[ns]\n",
      " 1   open    3060 non-null   float64       \n",
      " 2   high    3060 non-null   float64       \n",
      " 3   low     3060 non-null   float64       \n",
      " 4   close   3060 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 137.7 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3436 entries, 0 to 3435\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    3436 non-null   datetime64[ns]\n",
      " 1   open    3436 non-null   float64       \n",
      " 2   high    3436 non-null   float64       \n",
      " 3   low     3436 non-null   float64       \n",
      " 4   close   3436 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 134.3 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5662 entries, 0 to 5661\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    5662 non-null   datetime64[ns]\n",
      " 1   open    5610 non-null   float64       \n",
      " 2   high    5610 non-null   float64       \n",
      " 3   low     5610 non-null   float64       \n",
      " 4   close   5610 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 221.3 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4697 entries, 0 to 4696\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    4697 non-null   datetime64[ns]\n",
      " 1   open    4697 non-null   float64       \n",
      " 2   high    4697 non-null   float64       \n",
      " 3   low     4697 non-null   float64       \n",
      " 4   close   4697 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 183.6 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2788 entries, 0 to 2787\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    2788 non-null   datetime64[ns]\n",
      " 1   open    2749 non-null   float64       \n",
      " 2   high    2749 non-null   float64       \n",
      " 3   low     2749 non-null   float64       \n",
      " 4   close   2749 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 109.0 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5620 entries, 0 to 5619\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   date    5620 non-null   datetime64[ns]\n",
      " 1   open    5620 non-null   float64       \n",
      " 2   high    5620 non-null   float64       \n",
      " 3   low     5620 non-null   float64       \n",
      " 4   close   5620 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 219.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check whether the columns have been converted to float\n",
    "for df in dataframes.values():\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to handle the missing values present in each DataFrame. Since these stock datasets will eventually be fed into a Vector Auto-regressive (VAR) model, it is quite important to choose a data imputation method that will preserve the temporal and cross-sectional relationships among countries.\n",
    "\n",
    "Data imputation will be performed in two separate sections since one of our following step will create more missing values. A suitable method is chosen for each step, details are below:\n",
    "\n",
    "1. Using linear interpolation to fill in the NA values originally presented in each dataset.\n",
    "2. Dividing the imputed datasets from step 1 into 5 windows as defined in the Window Extraction step, resulting in 5 dictionaries containing DataFrames of countries with available data in each period.\n",
    "3. The goal of this step is to normalize the date range in each time window since every country has their own public holidays, making the number of available daily trading data different for each country. Performing this step will ensure that holidays data will be included and the number of daily data available for each country within a time window is the same. For each DataFrame in a time window, map the available data into a new empty DataFrame with a date column containing every business days (excluding public holidays) of the associated period. Then, missing data from the mapped DataFrame will be imputed using forward filling method.\n",
    "4. Calculate daily volatility for each DataFrame in every time window.\n",
    "5. Create a single DataFrame for each time window with each column representing a country's volatility calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for philippines:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for singapore:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for india:\n",
      "date      0\n",
      "open     39\n",
      "high     39\n",
      "low      39\n",
      "close    39\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for uk:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for mexico:\n",
      "date      0\n",
      "open     49\n",
      "high     49\n",
      "low      49\n",
      "close    49\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for japan:\n",
      "date      0\n",
      "open     93\n",
      "high     93\n",
      "low      93\n",
      "close    93\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for vietnam:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for korea:\n",
      "date      0\n",
      "open     66\n",
      "high     66\n",
      "low      66\n",
      "close    66\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for thailand:\n",
      "date      0\n",
      "open     73\n",
      "high     73\n",
      "low      73\n",
      "close    73\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for brazil:\n",
      "date       0\n",
      "open     462\n",
      "high     462\n",
      "low      462\n",
      "close    462\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for malaysia:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for switzerland:\n",
      "date      0\n",
      "open     52\n",
      "high     52\n",
      "low      52\n",
      "close    52\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for china:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for russia:\n",
      "date      0\n",
      "open     39\n",
      "high     39\n",
      "low      39\n",
      "close    39\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for us:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_missing_values(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in the dataframes using linear interpolation method\n",
    "dataframes = linear_interpolation(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for philippines:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for singapore:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for india:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for uk:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for mexico:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for japan:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for vietnam:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for korea:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for thailand:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for brazil:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for malaysia:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for switzerland:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for china:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for russia:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for us:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether the missing values have been imputed\n",
    "check_missing_values(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Window extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will perform window extraction for the following time period:\n",
    "- **Window 1:** 02.01.2002 to 17.09.2007\n",
    "- **Window 2:** 18.09.2007 to 27.10.2011 (India's dataset starts from 18.09.2007)\n",
    "- **Window 3:** 28.10.2011 to 31.12.2018 (Philippines's dataset starts from 28.10.2011)\n",
    "- **Window 4:** 02.01.2019 to 31.12.2022 \n",
    "- **Window 5:** 02.01.2023 to 30.04.2024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02.01.2002 - 17.09.2007: Recovery period from Dot-com Bubble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'philippines': False,\n",
       " 'singapore': False,\n",
       " 'india': False,\n",
       " 'uk': True,\n",
       " 'mexico': True,\n",
       " 'japan': True,\n",
       " 'vietnam': False,\n",
       " 'korea': True,\n",
       " 'thailand': True,\n",
       " 'brazil': False,\n",
       " 'malaysia': False,\n",
       " 'switzerland': True,\n",
       " 'china': True,\n",
       " 'russia': False,\n",
       " 'us': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether each DataFrame contains data from 02.01.2002 to 17.09.2007\n",
    "window_1_start_date = '2002-01-02'\n",
    "window_1_end_date = '2007-09-17'\n",
    "check_date_range(dataframes, window_1_start_date, window_1_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following countries do not have data available for this time period:\n",
    "1. Philippine\n",
    "2. Singapore\n",
    "3. India\n",
    "4. Vietnam\n",
    "5. Brazil\n",
    "6. Malaysia\n",
    "7. Russia\n",
    "\n",
    "These 7 countries will be be considered in VAR experiment for this time period, with the addition of China since it only has data from 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'philippines': Empty DataFrame\n",
       " Columns: [date, open, high, low, close]\n",
       " Index: [],\n",
       " 'singapore': Empty DataFrame\n",
       " Columns: [date, open, high, low, close]\n",
       " Index: [],\n",
       " 'india': Empty DataFrame\n",
       " Columns: [date, open, high, low, close]\n",
       " Index: [],\n",
       " 'uk':            date    open    high     low   close\n",
       " 0    2002-02-01  5164.8  5227.4  5164.8  5189.7\n",
       " 1    2005-02-01  4852.3  4906.2  4852.3  4906.2\n",
       " 2    2006-02-01  5760.3  5816.0  5746.2  5801.6\n",
       " 3    2007-02-01  6203.1  6300.3  6203.1  6282.2\n",
       " 17   2002-03-01  5101.0  5172.3  5101.0  5169.0\n",
       " ...         ...     ...     ...     ...     ...\n",
       " 5568 2005-10-31  5213.4  5318.4  5213.4  5317.3\n",
       " 5569 2006-10-31  6126.8  6149.9  6110.9  6129.2\n",
       " 5582 2002-12-31  3900.6  3949.1  3890.7  3940.4\n",
       " 5583 2003-12-31  4470.4  4491.8  4470.4  4476.9\n",
       " 5584 2004-12-31  4820.1  4822.3  4801.1  4814.3\n",
       " \n",
       " [1443 rows x 5 columns],\n",
       " 'mexico':            date          open          high           low         close\n",
       " 0    2002-01-02   6386.180176   6415.850098   6382.020020   6410.049805\n",
       " 1    2002-01-03   6410.089844   6603.750000   6410.089844   6603.750000\n",
       " 2    2002-01-04   6604.069824   6655.689941   6604.069824   6612.080078\n",
       " 3    2002-01-07   6615.870117   6635.950195   6556.310059   6565.439941\n",
       " 4    2002-01-08   6566.089844   6642.310059   6561.850098   6641.140137\n",
       " ...         ...           ...           ...           ...           ...\n",
       " 1479 2007-09-11  29893.179688  30307.359375  29892.439453  30191.140625\n",
       " 1480 2007-09-12  30191.140625  30317.500000  30025.619141  30076.330078\n",
       " 1481 2007-09-13  30076.330078  30513.480469  30063.779297  30302.230469\n",
       " 1482 2007-09-14  30291.509766  30389.019531  30091.490234  30096.029297\n",
       " 1483 2007-09-17  30080.419922  30222.130859  29573.589844  29794.490234\n",
       " \n",
       " [1484 rows x 5 columns],\n",
       " 'japan':            date          open          high           low         close\n",
       " 0    2002-01-04  10631.000000  10871.490234  10617.080078  10871.490234\n",
       " 1    2002-01-07  10803.450195  10979.919922  10803.450195  10942.360352\n",
       " 2    2002-01-08  10841.969727  10843.259766  10662.250000  10695.599609\n",
       " 3    2002-01-09  10661.250000  10747.599609  10638.429688  10663.980469\n",
       " 4    2002-01-10  10652.169922  10710.480469  10494.349609  10538.429688\n",
       " ...         ...           ...           ...           ...           ...\n",
       " 1471 2007-09-10  15906.519531  15906.519531  15651.830078  15764.969727\n",
       " 1472 2007-09-11  15787.860352  15940.379883  15610.650391  15877.669922\n",
       " 1473 2007-09-12  15978.780273  16032.259766  15731.320313  15797.599609\n",
       " 1474 2007-09-13  15886.759766  15931.089844  15802.360352  15821.190430\n",
       " 1475 2007-09-14  15895.049805  16142.080078  15877.089844  16127.419922\n",
       " \n",
       " [1476 rows x 5 columns],\n",
       " 'vietnam': Empty DataFrame\n",
       " Columns: [date, open, high, low, close]\n",
       " Index: [],\n",
       " 'korea':            date         open         high          low        close\n",
       " 0    2002-01-02   698.000000   725.059998   690.359985   724.950012\n",
       " 1    2002-01-03   726.760010   735.770020   722.349976   727.659973\n",
       " 2    2002-01-04   744.460022   750.830017   739.140015   747.719971\n",
       " 3    2002-01-07   748.539978   757.809998   735.549988   751.479980\n",
       " 4    2002-01-08   747.210022   750.890015   734.760010   734.760010\n",
       " ...         ...          ...          ...          ...          ...\n",
       " 1463 2007-09-11  1844.680054  1851.349976  1827.900024  1847.359985\n",
       " 1464 2007-09-12  1853.939941  1855.310059  1813.329956  1813.520020\n",
       " 1465 2007-09-13  1817.869995  1848.989990  1805.880005  1848.020020\n",
       " 1466 2007-09-14  1856.170044  1875.949951  1849.369995  1870.020020\n",
       " 1467 2007-09-17  1873.650024  1876.040039  1853.449951  1871.680054\n",
       " \n",
       " [1468 rows x 5 columns],\n",
       " 'thailand':            date        open        high         low       close\n",
       " 0    2002-01-02  304.000000  305.200012  302.380005  305.190002\n",
       " 1    2002-01-03  305.850006  312.429993  305.850006  312.049988\n",
       " 2    2002-01-04  316.190002  318.010010  314.299988  315.730011\n",
       " 3    2002-01-07  318.660004  321.149994  316.829987  317.690002\n",
       " 4    2002-01-08  315.859985  318.799988  315.059998  318.640015\n",
       " ...         ...         ...         ...         ...         ...\n",
       " 1457 2007-09-11  800.530029  804.330017  796.090027  801.539978\n",
       " 1458 2007-09-12  807.390015  808.059998  801.909973  802.000000\n",
       " 1459 2007-09-13  803.270020  808.510010  802.789978  807.099976\n",
       " 1460 2007-09-14  810.419983  816.820007  810.190002  811.950012\n",
       " 1461 2007-09-17  811.900024  812.049988  801.890015  802.650024\n",
       " \n",
       " [1462 rows x 5 columns],\n",
       " 'brazil': Empty DataFrame\n",
       " Columns: [date, open, high, low, close]\n",
       " Index: [],\n",
       " 'malaysia': Empty DataFrame\n",
       " Columns: [date, open, high, low, close]\n",
       " Index: [],\n",
       " 'switzerland':            date         open         high          low        close\n",
       " 0    2002-01-02  6379.299805  6446.700195  6351.700195  6372.299805\n",
       " 1    2002-01-03  6392.500000  6405.700195  6346.500000  6380.899902\n",
       " 2    2002-01-04  6408.299805  6412.799805  6361.200195  6392.899902\n",
       " 3    2002-01-07  6407.000000  6460.100098  6379.000000  6384.399902\n",
       " 4    2002-01-08  6353.200195  6378.299805  6313.899902  6313.899902\n",
       " ...         ...          ...          ...          ...          ...\n",
       " 1474 2007-09-11  8674.299805  8731.200195  8657.599609  8717.200195\n",
       " 1475 2007-09-12  8733.900391  8795.700195  8696.200195  8784.000000\n",
       " 1476 2007-09-13  8765.200195  8891.599609  8734.599609  8878.000000\n",
       " 1477 2007-09-14  8827.099609  8839.900391  8727.400391  8772.599609\n",
       " 1478 2007-09-17  8740.599609  8745.500000  8658.200195  8685.799805\n",
       " \n",
       " [1479 rows x 5 columns],\n",
       " 'china':            date     open     high      low    close\n",
       " 0    2005-02-01   953.33   965.48   952.74   955.95\n",
       " 1    2007-02-01  2350.60  2410.42  2310.57  2395.17\n",
       " 13   2005-03-01  1039.35  1042.74  1031.17  1035.93\n",
       " 14   2006-03-01  1051.76  1057.69  1049.64  1056.62\n",
       " 15   2007-03-01  2550.26  2550.33  2439.50  2473.54\n",
       " ...         ...      ...      ...      ...      ...\n",
       " 4658 2005-08-31   915.12   928.21   911.50   927.92\n",
       " 4659 2006-08-31  1335.59  1342.79  1331.29  1338.69\n",
       " 4660 2007-08-31  5255.09  5307.42  5219.99  5296.81\n",
       " 4673 2005-10-31   866.62   878.41   866.15   876.28\n",
       " 4674 2006-10-31  1447.06  1464.48  1447.06  1464.47\n",
       " \n",
       " [657 rows x 5 columns],\n",
       " 'russia': Empty DataFrame\n",
       " Columns: [date, open, high, low, close]\n",
       " Index: [],\n",
       " 'us':            date         open         high          low        close\n",
       " 0    2002-01-02  1148.079956  1154.670044  1136.229980  1154.670044\n",
       " 1    2002-01-03  1154.670044  1165.270020  1154.010010  1165.270020\n",
       " 2    2002-01-04  1165.270020  1176.550049  1163.420044  1172.510010\n",
       " 3    2002-01-07  1172.510010  1176.969971  1163.550049  1164.890015\n",
       " 4    2002-01-08  1164.890015  1167.599976  1157.459961  1160.709961\n",
       " ...         ...          ...          ...          ...          ...\n",
       " 1432 2007-09-11  1451.689941  1472.479980  1451.689941  1471.489990\n",
       " 1433 2007-09-12  1471.099976  1479.500000  1465.750000  1471.560059\n",
       " 1434 2007-09-13  1471.469971  1489.579956  1471.469971  1483.949951\n",
       " 1435 2007-09-14  1483.949951  1485.989990  1473.180054  1484.250000\n",
       " 1436 2007-09-17  1484.239990  1484.239990  1471.819946  1476.650024\n",
       " \n",
       " [1437 rows x 5 columns]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_window_1 = extract_date_range(dataframes, window_1_start_date, window_1_end_date)\n",
    "\n",
    "# Check whether each DataFrame contains data from 02.01.2002 to 17.09.2007\n",
    "dfs_window_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove countries that do not have data for the specified date range (02.01.2002 to 17.09.2007) from the dictionary\n",
    "for key in list(dfs_window_1.keys()):\n",
    "  if dfs_window_1[key].empty:\n",
    "    del dfs_window_1[key]\n",
    "\n",
    "# Remove the 'china' DataFrame from the dictionary\n",
    "del dfs_window_1['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs_window_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dfs_window_1 to a new date range\n",
    "dfs_window_1 = map_date_range(dfs_window_1, window_1_start_date, window_1_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for philippines:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for singapore:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for india:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for uk:\n",
      "date      0\n",
      "open     46\n",
      "high     46\n",
      "low      46\n",
      "close    46\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for mexico:\n",
      "date     0\n",
      "open     5\n",
      "high     5\n",
      "low      5\n",
      "close    5\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for japan:\n",
      "date      0\n",
      "open     13\n",
      "high     13\n",
      "low      13\n",
      "close    13\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for vietnam:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for korea:\n",
      "date      0\n",
      "open     21\n",
      "high     21\n",
      "low      21\n",
      "close    21\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for thailand:\n",
      "date      0\n",
      "open     27\n",
      "high     27\n",
      "low      27\n",
      "close    27\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for brazil:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for malaysia:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for switzerland:\n",
      "date      0\n",
      "open     10\n",
      "high     10\n",
      "low      10\n",
      "close    10\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for russia:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for us:\n",
      "date      0\n",
      "open     52\n",
      "high     52\n",
      "low      52\n",
      "close    52\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dfs_window_1\n",
    "check_missing_values(dfs_window_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annguyen/Documents/Master Degree/Dissertation/Submissions/Experiments/functions/preprocessing.py:42: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=\"ffill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in dfs_window_1 using forward fill method\n",
    "dfs_window_1 = forward_fill(dfs_window_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for philippines:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for singapore:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for india:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for uk:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for mexico:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for japan:\n",
      "date     0\n",
      "open     2\n",
      "high     2\n",
      "low      2\n",
      "close    2\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for vietnam:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for korea:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for thailand:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for brazil:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for malaysia:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for switzerland:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for russia:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for us:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dfs_window_1\n",
    "check_missing_values(dfs_window_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Japan still has 2 missing values. These missing values are probably the first two days of the period. Let's use backward fill to handle these two missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annguyen/Documents/Master Degree/Dissertation/Submissions/Experiments/functions/preprocessing.py:54: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dfs_window_1 = backward_fill(dfs_window_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for philippines:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for singapore:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for india:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for uk:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for mexico:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for japan:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for vietnam:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for korea:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for thailand:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for brazil:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for malaysia:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for switzerland:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for russia:\n",
      "date        0\n",
      "open     1489\n",
      "high     1489\n",
      "low      1489\n",
      "close    1489\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values for us:\n",
      "date     0\n",
      "open     0\n",
      "high     0\n",
      "low      0\n",
      "close    0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dfs_window_1 again to make sure the missing values have been imputed\n",
    "check_missing_values(dfs_window_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the volatility for each country\n",
    "dfs_window_1 = calculate_volatility(dfs_window_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volatility DataFrame for window 1\n",
    "df_window_1 = create_volatility_df(dfs_window_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>philippines</th>\n",
       "      <th>singapore</th>\n",
       "      <th>india</th>\n",
       "      <th>uk</th>\n",
       "      <th>mexico</th>\n",
       "      <th>japan</th>\n",
       "      <th>vietnam</th>\n",
       "      <th>korea</th>\n",
       "      <th>thailand</th>\n",
       "      <th>brazil</th>\n",
       "      <th>malaysia</th>\n",
       "      <th>switzerland</th>\n",
       "      <th>russia</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.955641</td>\n",
       "      <td>5.943942</td>\n",
       "      <td>19.791884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.994083</td>\n",
       "      <td>12.228153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.881106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.664973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.456913</td>\n",
       "      <td>21.957613</td>\n",
       "      <td>19.791884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.862243</td>\n",
       "      <td>17.854374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.890345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.171472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.755441</td>\n",
       "      <td>10.922138</td>\n",
       "      <td>19.791884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.446678</td>\n",
       "      <td>16.412695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.019224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.936110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.064627</td>\n",
       "      <td>14.299720</td>\n",
       "      <td>16.900705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.700360</td>\n",
       "      <td>18.717009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.311078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.120694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.443547</td>\n",
       "      <td>10.420974</td>\n",
       "      <td>17.203246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.994618</td>\n",
       "      <td>12.864248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.207484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.487229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  philippines  singapore  india         uk     mexico      japan  \\\n",
       "0 2002-01-02          NaN        NaN    NaN  17.955641   5.943942  19.791884   \n",
       "1 2002-01-03          NaN        NaN    NaN  18.456913  21.957613  19.791884   \n",
       "2 2002-01-04          NaN        NaN    NaN  12.755441  10.922138  19.791884   \n",
       "3 2002-01-07          NaN        NaN    NaN  19.064627  14.299720  16.900705   \n",
       "4 2002-01-08          NaN        NaN    NaN  15.443547  10.420974  17.203246   \n",
       "\n",
       "   vietnam      korea   thailand  brazil  malaysia  switzerland  russia  \\\n",
       "0      NaN  51.994083  12.228153     NaN       NaN    20.881106     NaN   \n",
       "1      NaN  25.862243  17.854374     NaN       NaN    12.890345     NaN   \n",
       "2      NaN  21.446678  16.412695     NaN       NaN    11.019224     NaN   \n",
       "3      NaN  41.700360  18.717009     NaN       NaN    17.311078     NaN   \n",
       "4      NaN  22.994618  12.864248     NaN       NaN    12.207484     NaN   \n",
       "\n",
       "          us  \n",
       "0  21.664973  \n",
       "1   8.171472  \n",
       "2  13.936110  \n",
       "3  14.120694  \n",
       "4  11.487229  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the first 5 rows of the volatility DataFrame\n",
    "df_window_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing ADF Stationarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingDataError",
     "evalue": "exog contains inf or nans",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDataError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m df_window_1\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43madf_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_window_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/functions/preprocessing.py:114\u001b[0m, in \u001b[0;36madf_test\u001b[0;34m(df, variable_name, significance_level)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madf_test\u001b[39m(\n\u001b[1;32m    110\u001b[0m     df: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m    111\u001b[0m     variable_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolatility\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m     significance_level: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m    113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43madfuller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Extract ADF results\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     adf_statistic \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/tsa/stattools.py:326\u001b[0m, in \u001b[0;36madfuller\u001b[0;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# 1 for level\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# search for lag length with smallest information criteria\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Note: use the same number of observations to have comparable IC\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# aic and bic: smaller is better\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m regresults:\n\u001b[0;32m--> 326\u001b[0m     icbest, bestlag \u001b[38;5;241m=\u001b[39m \u001b[43m_autolag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxdshort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullRHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautolag\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     icbest, bestlag, alres \u001b[38;5;241m=\u001b[39m _autolag(\n\u001b[1;32m    331\u001b[0m         OLS,\n\u001b[1;32m    332\u001b[0m         xdshort,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m         regresults\u001b[38;5;241m=\u001b[39mregresults,\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/tsa/stattools.py:132\u001b[0m, in \u001b[0;36m_autolag\u001b[0;34m(mod, endog, exog, startlag, maxlag, method, modargs, fitargs, regresults)\u001b[0m\n\u001b[1;32m    130\u001b[0m method \u001b[38;5;241m=\u001b[39m method\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(startlag, startlag \u001b[38;5;241m+\u001b[39m maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 132\u001b[0m     mod_instance \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mlag\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     results[lag] \u001b[38;5;241m=\u001b[39m mod_instance\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:924\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    923\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:749\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    752\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:203\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/base/model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/base/model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/base/model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/base/data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/base/data.py:88\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documents/Master Degree/Dissertation/Submissions/Experiments/venv/lib/python3.9/site-packages/statsmodels/base/data.py:134\u001b[0m, in \u001b[0;36mModelData._handle_constant\u001b[0;34m(self, hasconst)\u001b[0m\n\u001b[1;32m    132\u001b[0m exog_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(exog_max)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingDataError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexog contains inf or nans\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    135\u001b[0m exog_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    136\u001b[0m const_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(exog_max \u001b[38;5;241m==\u001b[39m exog_min)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mMissingDataError\u001b[0m: exog contains inf or nans"
     ]
    }
   ],
   "source": [
    "for country in df_window_1.drop(columns=['date']).columns:\n",
    "    adf_test(df_window_1, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window_1 = stationary_transformation(df_window_1, 'switzerland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window_1 = df_window_1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform ADF test again to make sure that the data is stationary\n",
    "for country in df_window_1.drop(columns=['date']).columns:\n",
    "    adf_test(df_window_1, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18.09.2007 - 27.10.2011: Global Financial Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether each DataFrame contains data from 18.09.2007 to 27.10.2011\n",
    "window_2_start_date = '2007-09-18'\n",
    "window_2_end_date = '2011-10-27'\n",
    "check_date_range(dataframes, window_2_start_date, window_2_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most countries have data from 18.09.2007 to 27.10.2011, with the exception of Philippines and Russia. In addition, there are Singapore, Vietnam, Brazil, and Malaysia which only has data from 07.03.2011, 05.01.2009, 24.02.2010, and 20.05.2010 respectively. These 4 countries will be remove from this time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_window_2 = extract_date_range(dataframes, window_2_start_date, window_2_end_date)\n",
    "\n",
    "# Check whether each DataFrame contains data from 18.09.2007 to 27.10.2011\n",
    "dfs_window_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove countries that do not have data for the specified date range (02.01.2002 to 17.09.2007) from the dictionary\n",
    "addtional_columns = ['singapore', 'vietnam', 'brazil', 'malaysia']\n",
    "\n",
    "for key in list(dfs_window_2.keys()):\n",
    "  if dfs_window_2[key].empty or key in addtional_columns:\n",
    "    del dfs_window_2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs_window_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dfs_window_2 to a new date range\n",
    "dfs_window_2 = map_date_range(dfs_window_2, window_2_start_date, window_2_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_2\n",
    "check_missing_values(dfs_window_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward filling to fill in the missing values in dfs_window_2\n",
    "dfs_window_2 = forward_fill(dfs_window_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_2\n",
    "check_missing_values(dfs_window_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the volatility for each country\n",
    "dfs_window_2 = calculate_volatility(dfs_window_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volatility DataFrame for window 2\n",
    "df_window_2 = create_volatility_df(dfs_window_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing ADF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in df_window_2.drop(columns=['date']).columns:\n",
    "    adf_test(df_window_2, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the variables are stationary, no transformation is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28.10.2011 - 31.12.2018: Recovery from Global Financial Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether each DataFrame contains data from 28.10.2011 to 31.12.2018\n",
    "window_3_start_date = '2011-10-28'\n",
    "window_3_end_date = '2018-12-31'\n",
    "check_date_range(dataframes, window_3_start_date, window_3_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most DataFrame appears to have data from 28.10.2011 to 31.12.2018, except from Russia, which only has data from 2013. Russia will be removed from this time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_window_3 = extract_date_range(dataframes, window_3_start_date, window_3_end_date)\n",
    "\n",
    "# Check whether each DataFrame contains data from 28.10.2011 - 31.12.2018\n",
    "dfs_window_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfs_window_3['russia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs_window_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dfs_window_3 to a new date range\n",
    "dfs_window_3 = map_date_range(dfs_window_3, window_3_start_date, window_3_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_3\n",
    "check_missing_values(dfs_window_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform forward fill to handle the missing values\n",
    "dfs_window_3 = forward_fill(dfs_window_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_3 to make sure there is no missing values left\n",
    "check_missing_values(dfs_window_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the volatility for each country\n",
    "dfs_window_3 = calculate_volatility(dfs_window_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volatility DataFrame for window 3\n",
    "df_window_3 = create_volatility_df(dfs_window_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing ADF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in df_window_3.drop(columns=['date']).columns:\n",
    "    adf_test(df_window_3, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables are stationary. Therefore, no transformation is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02.01.2019 - 31.12.2022: COVID-19 pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether each DataFrame contains data from 02.01.2019 to 31.12.2022\n",
    "window_4_start_date = '2019-01-02'\n",
    "window_4_end_date = '2022-12-31'\n",
    "check_date_range(dataframes, window_4_start_date, window_4_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_window_4 = extract_date_range(dataframes, window_4_start_date, window_4_end_date)\n",
    "\n",
    "# Check whether each DataFrame contains data from 02.01.2019 to 31.12.2022\n",
    "dfs_window_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dfs_window_4 to a new date range\n",
    "dfs_window_4 = map_date_range(dfs_window_4, window_4_start_date, window_4_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_4\n",
    "check_missing_values(dfs_window_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peforming forward filling imputation to handle the missing values\n",
    "dfs_window_4 = forward_fill(dfs_window_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_4 to make sure that there is no missing data left\n",
    "check_missing_values(dfs_window_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backward filling imputation to handle the remaining missing values\n",
    "dfs_window_4 = backward_fill(dfs_window_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_4 to make sure that there is no missing data left\n",
    "check_missing_values(dfs_window_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the volatility for each country\n",
    "dfs_window_4 = calculate_volatility(dfs_window_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volatility DataFrame for window 4\n",
    "df_window_4 = create_volatility_df(dfs_window_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform ADF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in df_window_4.drop(columns=['date']).columns:\n",
    "    adf_test(df_window_4, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables are stationary. Therefore, no transformation is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02.01.2023 - 30.04.2024: Recovery from COVID-19 pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether each DataFrame contains data from 02.01.2023 to 30.04.2024\n",
    "window_5_start_date = '2023-01-02'\n",
    "window_5_end_date = '2024-04-30'\n",
    "check_date_range(dataframes, window_5_start_date, window_5_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_window_5 = extract_date_range(dataframes, window_5_start_date, window_5_end_date)\n",
    "\n",
    "# Check whether each DataFrame contains data from 02.01.2023 to 30.04.2024\n",
    "dfs_window_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dfs_window_4 to a new date range\n",
    "dfs_window_5 = map_date_range(dfs_window_5, window_5_start_date, window_5_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_5\n",
    "check_missing_values(dfs_window_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward filling imputation\n",
    "dfs_window_5 = forward_fill(dfs_window_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_5 again to make sure there is no missing values left\n",
    "check_missing_values(dfs_window_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backward filling to handle the remaining missing values\n",
    "dfs_window_5 = backward_fill(dfs_window_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dfs_window_5 again to make sure there is no missing values left\n",
    "check_missing_values(dfs_window_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the volatility for each country\n",
    "dfs_window_5 = calculate_volatility(dfs_window_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volatility DataFrame for window 5\n",
    "df_window_5 = create_volatility_df(dfs_window_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peform ADF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in df_window_5.drop(columns=['date']).columns:\n",
    "    adf_test(df_window_5, country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables are stationary. Therefore, no transformation is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of windowed volatility DataFrames\n",
    "windowed_vol_dfs = [df_window_1, df_window_2, df_window_3, df_window_4, df_window_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_volatility in enumerate(windowed_vol_dfs):\n",
    "    window = f\"window_{i+1}\"\n",
    "    print(f\"Window: {window}\")\n",
    "    print(df_volatility.drop(columns=['date']).corr(method='pearson'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Spillover Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_volatility in enumerate(windowed_vol_dfs):\n",
    "  window = f\"window_{i+1}\"\n",
    "  print(f\"Calculating spillover for {window}...\")\n",
    "  # Calculate the average spillover for each window\n",
    "  spillovers_table, lag_order, forecast_horizon = calculate_avg_spillover_table(\n",
    "      df_volatility.drop(columns=[\"date\"])\n",
    "  )\n",
    "  print(f\"Finished calculating spillover for {window}!\")\n",
    "  print(f\"Lag order: {lag_order}\")\n",
    "  print(f\"Forecast horizon: {forecast_horizon}\")\n",
    "  print(f\"Saving spillover table for {window}...\")\n",
    "  # Save the spillover table to a CSV file\n",
    "  spillovers_table.to_csv(\n",
    "      f\"output/var/{window}_spillover_table.csv\", index=True\n",
    "  )\n",
    "  print(f\"Finished saving spillover table for {window}!\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Net Pair-wise Spillover Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  window = f\"window_{i+1}\"\n",
    "  print(f\"Calculating net pair-wise spillover for {window}...\")\n",
    "  # Read the spillover table for the current window\n",
    "  spillover_table = pd.read_csv(f\"output/var/{window}_spillover_table.csv\", index_col=0)\n",
    "  # Calculate the net pair-wise spillover for the current window\n",
    "  normalized_spillover_table = spillover_table.drop(index=['Directional TO others', 'Total Spillover Index']).drop(columns=['Directional FROM others'])\n",
    "  net_pairwise_spillover_table = calculate_net_pairwise_spillover_table(normalized_spillover_table)\n",
    "  print(f\"Finished calculating net pair-wise spillover for {window}!\")\n",
    "\n",
    "  print(net_pairwise_spillover_table)\n",
    "  print()\n",
    "  # print(f\"Saving net pair-wise spillover table for {window}...\")\n",
    "  # # Save the net pair-wise spillover table to a CSV file\n",
    "  # net_pairwise_spillover_table.to_csv(f\"output/var/{window}_net_pairwise_spillover_table.csv\", index=True)\n",
    "  # print(f\"Finished saving net pair-wise spillover table for {window}!\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAR Project",
   "language": "python",
   "name": "venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
